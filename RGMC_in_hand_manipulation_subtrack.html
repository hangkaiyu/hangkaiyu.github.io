<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"https://www.w3.org/TR/html4/loose.dtd">
<html xmlns="https://www.w3.org/1999/xhtml">
    <head>
        <!-- <meta https-equiv="Content-Type" content="text/html; charset=utf-8" /> -->
        <title>Kaiyu Hang</title>
        <link rel="shortcut icon" href="pics/RiceIcon.png">
        <link rel="stylesheet" href="myStyle.css">
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
      <script> 
        $(function(){
          $("#commonHeader").load("header.html"); 
        });
        $(function(){
          $("#disclaimer").load("disclaimer.html"); 
        });
      </script> 
        
    </head>
    <body>
      

      <div id="wrapper">
      
        <div id="content">
        <div id="commonHeader"></div>


          
          <hr>

            <div id="inner">
            <center>
            <font size="4"><b>
            <a href="https://sites.google.com/view/rgmc2025" target="_blank">
                The 10th Robotic Grasping and Manipulation Competition (RGMC)
            </a>
            </b></font><br>
               IEEE-RAS International Conference on Robotics and Automation (ICRA)<br>
               Atlanta, USA, May 19-23, 2024<br>
               <br>
               <br>
           </center>





                

                <font size="4"><b>Essential Skill Sub-Track 2: In-Hand Manipulation</b></font><br>
                <b>Organizers:</b> Kaiyu Hang (Rice), Podshara Chanrungmaneekul (Rice), Joshua T. Grace (Yale), Andrew S. Morgan (RAI Institute)<br>
		        <!-- <b>Judges:</b> , Kapil Katyal (Amazon Robotics)<br> -->
		        <b>Timeline:</b> May 19 (Dry Run), May 20-21 (Competition Day)<br>
                <b>Competition GitHub Repository:</b> <a href="https://github.com/Rice-RobotPI-Lab/rgmc_in_hand_manipulation_2025" target="_blank">rgmc_in_hand_manipulation_2025</a>.<br><br>

                &nbsp&nbsp&nbsp&nbsp
                The focus of this subtrack is on the in-hand manipulation skills of different robot hands and various manipulation planning and control algorithms. Competition information and scoring criteria are detailed below.<br><br>

                <center><b><u>I. Workspace Setup</u></b></center>
                &nbsp&nbsp&nbsp&nbsp
                Every team will bring their own robot hand for the competition. There is no restriction on the design of the hardware. The robot hands can be commercial models, open source models, or models designed the participating teams. The robot hand will be installed on a stationary mount designed by the teams, e.g., the mount can be a stationary frame that can stably hold the robot hand. The teams can change the hand mounting poses as needed for different tasks. An example hand setup is shown in Fig. 1. <b>Note: regardless of the mount used to fix the robot hand, for all manipulation tasks in this competition, only hand actions are allowed. Even if a team decides to mount their robot hand on a robot arm, the robot arm has to be stationary during the task executions.</b><br>
                

                <figure>
                    <center>
                        <img style="margin-bottom:20px; margin-right:20px" align = "center" width="40%" src="pics/RGMC_example_setup.png"/>
                        <figcaption>Fig. 1: An example hand setup with a camera on top of the object being manipulated.</figcaption>
                    </center>
                </figure>


                <center><b><u>II. Sensors</u></b></center> 
                &nbsp&nbsp&nbsp&nbsp
                Every team is required to use at least one camera that can support the use of 
                <a href="https://wiki.ros.org/apriltag_ros" target="_blank">
                    apriltag_ros.
                </a>
                As exemplified in Fig. 1, the object being manipulated will be tracked by this camera through an apriltag. As will be detailed below, the scoring of the competition is heavily relying on the apriltags, and the manipulation goals are specified in the camera's view/frame based on the readings of apriltags. Therefore, we do not require specific hand-camera calibration.<br>


                &nbsp&nbsp&nbsp&nbsp
                There is no restriction on the sensors used by teams. Vision sensors, tactile sensors, force/torque sensors, etc., are all welcome to be included in the setups.<br><br>


                <center><b><u>III. Hand Control</u></b></center> 
                &nbsp&nbsp&nbsp&nbsp
                During the competition, after a team has finished setting up all necessary equipment, the robot hand should run fully autonomously to complete the competition tasks. No teleopration or human intervention will be allowed. In cases where human interventions are needed for safety reasons, that specific run of the competition will be regarded a failure. <br>
                <br>

                <center><b><u>IV. Competition Objects</u></b></center> 

                &nbsp&nbsp&nbsp&nbsp
                To make it easy for every team to use the same objects for system development and test, this competition will use 3D printed objects. As shown in Fig. 2, three cynlinders of different sizes and two cubes with "A, B, C, D, E, F" written on different facets, together with their CAD models, are provided.<br>

                &nbsp&nbsp&nbsp&nbsp
                The cylinders are 60mm, 80mm, 100mm in diameters and 80mm tall. The cubes are of sizes 50*50*50 mm<sup>3</sup> and 90*90*90 mm<sup>3</sup>. On the cynlinder objects, the is a 30*30*1mm<sup>3</sup> reserved space to attach the apriltags. On the cube object, there are one or four of such reserved spaces on each facet. Please read the details in the GitHub Repository about how to track those objects.<br>


                &nbsp&nbsp&nbsp&nbsp
                The models can be downloaded from the
                <a href="https://github.com/Rice-RobotPI-Lab/rgmc_in_hand_manipulation_2025" target="_blank">
                rgmc_in_hand_manipulation_2025 GitHub Repository.
                </a>
                In addition, there will be one "novel" object at the competition. This "novel" object will be an object of similar sizes to the ones we provided, and will be selected from the 
                <a href="https://www.ycbbenchmarks.com/object-models/" target="_blank">
                YCB Object and Model Set.
                </a>


                <figure>
                    <center>
                        <img style="margin-bottom:20px; margin-right:20px" align = "center" width="50%" src="pics/RGMC_objects.png"/>
                        <figcaption>Fig. 2: All objects used in the competition. The "novel" object is not shown here.</figcaption>
                    </center>
                </figure>


                <center><b><u>V. Competition Tasks</u></b></center>
                <b>Task A: Object Position Control for Waypoints Tracking</b><br>

                &nbsp&nbsp&nbsp&nbsp
                The robot hand will grasp a cylinderical object with an apriltag attached at the top of it. The grasp should be initialized by a human operator, e.g., a human operator can give the object into the robot hand. <u>Every team will need to grasp a cynliner of only one size of their choice. In this task, a "novel object" will be used in addition to the cynliders, so that every team will need to run their system 2 times on both the cynlinder and the "novel" objects. </u> The task is to manipulate the object in-hand so that the apriltag's motion will track given waypoints. <br> 


                &nbsp&nbsp&nbsp&nbsp
                The waypoints will be given as a sequence of positions relative to the initial object's (apriltag's) position. Once the initial grasp is stabilized, the object's position (in the camera's frame) will be set as (0, 0, 0). Thereafter, a sequence of waypoints (in the camera's frame relative to the aforementioned (0, 0, 0)), in the form of (x, y, z), will be given. The robot hand will then be tasked to fully autonomously move the object (apriltag) through those waypoints one by one. The waypoints are all limited within the range of <b>[-2.5cm, 2.5cm] * [-2.5cm, 2.5cm] * [-2.5cm, 2.5cm]</b> centered at the intial position of the grasped object. Example lists of waypoints are provided in the GitHub Repository.<br>

                &nbsp&nbsp&nbsp&nbsp
                Note: There will be two subtasks in Task A: Task A1 will focus on the speed and Task A2 will focus on the precision of in-hand manipulation position control. The scoring details can be found in Sec. VI.

                <br><br>


                <b>Task B: Object Re-orientation</b><br>
                &nbsp&nbsp&nbsp&nbsp
                Every team will grasp only <u>one size of the cube objects of their choice</u>. The grasp should be initialized by a human operator, e.g., a human operator can give the object into the robot hand. The task is to manipulate the object in-hand so that the cube object will rotate in-hand to match a sequence of required orientations as marked by the "A, B, C, D, E, F" letters.<br>

                &nbsp&nbsp&nbsp&nbsp
                Once the initial grasp is stabilized, the team will be given the sequence of letters to reach one by one. A target letter is considered reached if the apriltag on that target facet is posed to be sufficiently aligned with the "Z" direction of the camera's frame. Skipping letters is not allowed and will potentially make all subsequent letters fail.<br>

                &nbsp&nbsp&nbsp&nbsp
                Note: There will be two subtasks in Task B: Task B1 will focus on the speed and Task B2 will focus on the overall capability of in-hand manipulation orientation control. The scoring details can be found in Sec. VI.

                <br><br>

                <center><b><u>VI. Scoring and Ranking</u></b></center>
                &nbsp&nbsp&nbsp&nbsp
                By tracking the apriltag, the organizers will provide a ROS package (auto-evaluator) to automatically evaluate the manipulation performance. The ROS package is hosted on GitHub at: 
                <a href="https://github.com/Rice-RobotPI-Lab/rgmc_in_hand_manipulation_2025" target="_blank">
                rgmc_in_hand_manipulation_2025</a>.<br><br>



                &nbsp&nbsp&nbsp&nbsp
                <b>Task A1</b>: Every team will be given a list of known waypoints (same as what is already in the GitHub repo) before the competition. The auto-evaluator will record the motion trajectory of the object (apriltag) and check if the object has reached the given waypoint. In this task, a waypoint is considered as reached if the positional error is within 0.5cm. Every team will have a 20-seconds time budget for each goal waypoint. <br>

                &nbsp&nbsp&nbsp&nbsp
                The auto-evalutor will record the execution time for each waypoint successfully reached and use the sum to rank teams, i.e., faster is better. If the robot fails to reach a waypoint, e.g., did not reach within the time limit (20 seconds), did not reach within the precision tolerance (0.5cm), or dropped the object, the task will be terminated and a 20-seconds penalty will be recorded for that failed waypoint and each remaining waypoint.<br>

                &nbsp&nbsp&nbsp&nbsp
                There is only one set of waypoints (known) for Task A1 for both cylinder and novel objects, i.e., same waypoints for both objects. Every team can have only 1 run on each object.

                <br><br>

                &nbsp&nbsp&nbsp&nbsp
                <b>Task A2</b>: Every team will be given a list of unknown waypoints (different from what is already in the GitHub repo). The auto-evaluator will record the motion trajectory of the object (apriltag) and calculate the accumulated errors. Every team will have a 20-seconds time budget for each goal waypoint. If the robot does not signal to the auto-evaluator that a waypoint is reached within 20 seconds, the auto-evaluator will switch to the next waypoint automatically.<br>

                &nbsp&nbsp&nbsp&nbsp
                There is only one set of waypoints for Task A for both cylinder and novel objects, i.e., same waypoints for both objects. Every team can have 1 run on each object. <br>

                &nbsp&nbsp&nbsp&nbsp
                For incomplete runs, e.g., object drops, teams are ranked by: (total_error/number_of_waypoints_reached). All incomplete runs will be ranked lower than complete runs regardless of the error.<br><br>

                &nbsp&nbsp&nbsp&nbsp
                <b>Task B1</b>: Every team will be given a list of known facets (same as what is already in the GitHub repo) before the competition. The auto-evaluator will check if the object has reached the given facet. A waypoint is considered reached if the orientation of the cube's goal facet is within 0.5 rad facing the camera. Every team will have a 30-seconds time budget for each goal facet. <br>

                &nbsp&nbsp&nbsp&nbsp
                The auto-evalutor will record the execution time for each facet successfully reached and use the sum to rank teams, i.e., faster is better. If the robot fails to reach a facet, e.g., did not reach within the time limit (30 seconds), did not reach within the precision tolerance (0.5 rad), or dropped the object, the task will be terminated and a 30-seconds penalty will be recorded for that failed facet and each remaining facet.<br>

                &nbsp&nbsp&nbsp&nbsp
                There is only one set of target facets (known) for Task B1. The task file will assume that the grasped cube is initially configured to show the facet "A". Every team can have 1 run on their selected cube.<br><br>


                &nbsp&nbsp&nbsp&nbsp
                <b>Task B2</b>: Every team will be given a list of unknown facets (different from what is already in the GitHub repo). The auto-evaluator will simply check how many target facets have been reached. If multiple teams have the same number of successfully reached facets, they will be ranked by the accumulated execution time. Every team will have a 30-seconds time budget for each goal facet. If the robot does not signal to the auto-evaluator that a facet is reached within 30 seconds, the auto-evaluator will switch to the next facet automatically.<br>
                &nbsp&nbsp&nbsp&nbsp
                There is only one set of target facets for Task B2. The task file will assume that the grasped cube is initially configured to show the facet "A". Every team can have 1 run on their selected cube.<br><br>

                &nbsp&nbsp&nbsp&nbsp
                <b>Overall scoring and ranking</b>: The scoring system is fully ranking-based. For Task A1 and A2 on the cynliner objects, team #1 will collect 5 points, and the teams ranked after will collect 4 to 1 points. The same rule applies to Task A1 and A2 on the "novel" object. For Task B1 and B2, team #1 will collect 10 points, and the teams ranked after will collect 9 to 1 points. <br>

                &nbsp&nbsp&nbsp&nbsp
                Finally, the overall ranking of the teams will be based on the total points each team has collected from all tasks. Teams can check out this <a href="https://github.com/Rice-RobotPI-Lab/rgmc_in_hand_manipulation_2025/blob/main/Scoring%20In-hand%20Manipulation.pdf" type="application/octet-stream" target="_blank">
                example scoring sheet</a> for reference.<br><br>





                <center><b><u>VII. Frequently Asked Questions (Q&A)</u></b></center>
                <br>
                <b>Q</b>: Is it allowed to use arm motions or wrist motions to move the object?<br>
                <b>A</b>: No, the hand should be installed on a stationary mount. Only hand motions, e.g., finger motions, palm motions, are allowed to manipulate the object. If the object is manipulated by any motions external to the hand, the team's performance will not be evaluated.<br><br>

                <b>Q</b>: Can we design an end-effector that holds the object with a fixed grasp and then have the object manipulated by other joints, e.g., revolute or prismatic, external to the grasp?<br>
                <b>A</b>: Such designs will not be treated as in-hand manipulation, since essentially an "arm or wrist" is used to move the object. If you would like to make sure your hand design is going to be qualified at the competition, you are encouraged to contact the organizers as soon as possible to confirm.<br><br>

                <b>Q</b>: How is a grasp initialized for the competition?<br>
                <b>A</b>: A human operator should give the object into the robot hand and secure the grasp. There is no need for any autonomous grasping. Once the grasp is secured, the human operator should not touch the setup anymore.<br><br>

                

                <b>Q</b>: Is there any orientation requirement for the tags in Task A?<br>
                <b>A</b>: No, task A is only evaluated by the positional accuracy. The tag can be arbitrarily oriented in the grasp but it is your team's responsibility to ensure that the tag will be detected by the camera.<br><br>

                <b>Q</b>: Is it allowed to use a palm support, instead of a real grasp, to hold the object during manipulation?<br>
                <b>A</b>: Yes, palm support is allowed. Essentially, as long as the object is fully held and manipulated by the robot hand, it is considered a valid in-hand manipulation solution.<br><br>

                <b>Q</b>: Where should the camera be installed?<br>
                <b>A</b>: Every team can install the camera at anywhere within their robot hand's workspace as long as it can see the object being manipulated.<br><br>











               <br>
               

               <br>
               <br>



        </div>
        
      <div id="disclaimer"></div>
      </div>

      
    </div>

   




		
    </body>
</html>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90337871-1', 'auto');
  ga('send', 'pageview');

</script>
